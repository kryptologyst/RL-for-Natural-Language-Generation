# Environment Configuration
env:
  vocab_size: 1000
  max_length: 50
  min_length: 5
  reward_type: "length"  # "length", "coherence", "diversity", "custom"
  temperature: 1.0
  seed: 42

# RL Algorithm Configuration
rl:
  algorithm: "ppo"  # "reinforce", "ppo", "a2c"
  learning_rate: 3e-4
  gamma: 0.99
  lambda_gae: 0.95
  
  # PPO specific
  ppo_clip_ratio: 0.2
  ppo_epochs: 4
  value_loss_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5
  
  # Training parameters
  batch_size: 64
  buffer_size: 10000
  update_frequency: 10
  
  # Exploration
  temperature: 1.0
  epsilon_start: 1.0
  epsilon_end: 0.01
  epsilon_decay: 0.995
  
  # Device
  device: "auto"  # "auto", "cpu", "cuda", "mps"

# Model Configuration
model:
  model_type: "transformer"  # "transformer", "lstm"
  hidden_size: 256
  d_model: 256
  nhead: 8
  num_layers: 6
  dim_feedforward: 1024
  dropout: 0.1
  max_length: 100

# Training Configuration
training:
  num_episodes: 1000
  eval_frequency: 100
  log_dir: "logs"
  use_wandb: false
  wandb_project: "rl-text-generation"
  save_frequency: 100

# Evaluation Configuration
evaluation:
  num_eval_episodes: 50
  num_sample_texts: 10
  temperature: 0.8
  top_k: 50
  top_p: 0.9
